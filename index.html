<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Whisper & Phi-3 Transcription System</title>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.20.1/dist/ort.min.js"></script>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      display: flex;
      flex-direction: column;
      align-items: center;
      background-color: #f4f4f9;
    }
    header {
      background-color: #6200ea;
      color: white;
      padding: 1rem;
      width: 100%;
      text-align: center;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }
    main {
      display: flex;
      flex-direction: row;
      gap: 1rem;
      max-width: 1200px;
      width: 95%;
      padding: 1rem;
      margin: 1rem auto;
      background: white;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
      border-radius: 10px;
    }
    .left-panel, .right-panel {
      flex: 1;
      display: flex;
      flex-direction: column;
      gap: 1rem;
    }
    .model-status {
      padding: 1rem;
      margin: 1rem 0;
      border-radius: 5px;
      background-color: #f8f9fa;
    }
    .progress-container {
      width: 100%;
      background-color: #f0f0f0;
      border-radius: 4px;
      margin: 0.5rem 0;
    }
    .progress-bar {
      width: 0%;
      height: 20px;
      background-color: #4CAF50;
      border-radius: 4px;
      transition: width 0.3s ease-in-out;
    }
    select, button, textarea {
      width: 100%;
      padding: 0.5rem;
      margin: 0.5rem 0;
      font-size: 1rem;
      border: 1px solid #ccc;
      border-radius: 5px;
    }
    textarea {
      resize: none;
      height: 200px;
    }
    button {
      background-color: #007bff;
      color: white;
      cursor: pointer;
      transition: background-color 0.3s ease;
    }
    button:disabled {
      background-color: #cccccc;
      cursor: not-allowed;
    }
    canvas {
      width: 100%;
      height: 100px;
      border: 1px solid #ddd;
      border-radius: 5px;
    }
  </style>
</head>
<body>
  <header>
    <h1>Whisper & Phi-3 Transcription System</h1>
  </header>
  <main>
    <div class="left-panel">
      <div class="model-status">
        <h3>Model Status</h3>
        <div id="whisperStatus">
          <p>Whisper Model: <span id="whisperStatusText">Not loaded</span></p>
          <div class="progress-container">
            <div id="whisperProgress" class="progress-bar"></div>
          </div>
        </div>
        <div id="phiStatus">
          <p>Phi-3 Model: <span id="phiStatusText">Not loaded</span></p>
          <div class="progress-container">
            <div id="phiProgress" class="progress-bar"></div>
          </div>
        </div>
      </div>
      <div>
        <label for="whisperModel">Choose Whisper Model:</label>
        <select id="whisperModel">
          <option value="tiny">Tiny</option>
          <option value="small">Small</option>
          <option value="medium">Medium</option>
        </select>
      </div>
      <div>
        <canvas id="visualizer"></canvas>
        <div class="button-group">
          <button id="startRecord" disabled>Start Recording</button>
          <button id="stopRecord" disabled>Stop Recording</button>
        </div>
      </div>
      <div>
        <label for="transcription">Transcription:</label>
        <textarea id="transcription" contenteditable="true"></textarea>
      </div>
    </div>
    <div class="right-panel">
      <div>
        <label for="aiResponse">AI Response:</label>
        <textarea id="aiResponse" readonly></textarea>
      </div>
      <button id="processText" disabled>Process with Phi-3</button>
      <button id="copyJSON">Copy as JSON</button>
    </div>
  </main>

  <script type="module">
    import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.6.1';

    // Configure environment for Whisper models
    env.allowLocalModels = true;
    env.remoteModels = {
      'Xenova/whisper-tiny': 'https://huggingface.co/Xenova/whisper-tiny/resolve/main/',
      'Xenova/whisper-small': 'https://huggingface.co/Xenova/whisper-small/resolve/main/',
      'Xenova/whisper-medium': 'https://huggingface.co/Xenova/whisper-medium/resolve/main/'
    };

    // Force single-threaded mode to avoid SIMD/threading issues.
    ort.env.wasm.numThreads = 1;

    // Global variables
    let whisperModel;
    let phiModel;
    let mediaRecorder;
    let audioChunks = [];
    let audioContext;
    let analyser;

    // UI Elements
    const whisperStatusText = document.getElementById('whisperStatusText');
    const whisperProgress = document.getElementById('whisperProgress');
    const phiStatusText = document.getElementById('phiStatusText');
    const phiProgress = document.getElementById('phiProgress');
    const startRecordButton = document.getElementById('startRecord');
    const stopRecordButton = document.getElementById('stopRecord');
    const processTextButton = document.getElementById('processText');
    const transcriptionArea = document.getElementById('transcription');
    const aiResponseArea = document.getElementById('aiResponse');
    const visualizer = document.getElementById('visualizer');
    const copyJSONButton = document.getElementById('copyJSON');

    // Initialize Whisper model using transformers pipeline
    async function initWhisper() {
      try {
        const selectedModel = document.getElementById('whisperModel').value;
        const cacheKey = `whisper-${selectedModel}-downloaded`;

        whisperStatusText.textContent = localStorage.getItem(cacheKey) === 'true' ?
          'Loading cached Whisper model...' :
          'Downloading Whisper model...';

        whisperModel = await pipeline('automatic-speech-recognition', `Xenova/whisper-${selectedModel}`, {
          progress_callback: (progress) => {
            const percent = Math.round(progress.progress * 100);
            whisperProgress.style.width = `${percent}%`;
          }
        });

        localStorage.setItem(cacheKey, 'true');
        whisperStatusText.textContent = 'Whisper model ready';
        whisperProgress.style.width = '100%';
        startRecordButton.disabled = false;
      } catch (error) {
        console.error('Error initializing Whisper:', error);
        whisperStatusText.textContent = 'Error loading Whisper model';
      }
    }

    // Initialize Phi-3 model using the approach from the GitHub example
async function initPhi3() {
  try {
    phiStatusText.textContent = 'Downloading Phi-3 model files...';

    // Fetch the main model file
    const onnxResponse = await fetch(
      'https://huggingface.co/onnx-community/Phi-3.5-mini-instruct-onnx-web/resolve/main/onnx/model_q4f16.onnx'
    );
    if (!onnxResponse.ok) {
      throw new Error('Failed to fetch the main ONNX model file.');
    }
    const onnxBuffer = await onnxResponse.arrayBuffer();

    // Fetch the external data file
    const dataResponse = await fetch(
      'https://huggingface.co/onnx-community/Phi-3.5-mini-instruct-onnx-web/resolve/main/onnx/model_q4f16.onnx_data'
    );
    if (!dataResponse.ok) {
      throw new Error('Failed to fetch the external data file.');
    }
    const dataBuffer = await dataResponse.arrayBuffer();

    // Combine the two buffers into one.
    // This assumes that simply appending the dataBuffer to onnxBuffer produces a merged model.
    const mergedBuffer = new Uint8Array(onnxBuffer.byteLength + dataBuffer.byteLength);
    mergedBuffer.set(new Uint8Array(onnxBuffer), 0);
    mergedBuffer.set(new Uint8Array(dataBuffer), onnxBuffer.byteLength);

    phiStatusText.textContent = 'Loading merged Phi-3 model...';

    // Create the inference session using the merged buffer.
    phiModel = await ort.InferenceSession.create(mergedBuffer.buffer, {
      executionProviders: ['wasm'],
      graphOptimizationLevel: 'all',
      enableCpuMemArena: true,
      enableMemPattern: true,
      executionMode: 'sequential'
    });

    phiStatusText.textContent = 'Phi-3 model ready';
    phiProgress.style.width = '100%';
    processTextButton.disabled = false;
  } catch (error) {
    console.error('Error initializing Phi-3:', error);
    phiStatusText.textContent = `Error loading Phi-3 model: ${error.message}`;
  }
}


    // Audio recording and transcription functions remain largely the same.
    async function startRecording() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        analyser = audioContext.createAnalyser();
        const source = audioContext.createMediaStreamSource(stream);
        source.connect(analyser);

        mediaRecorder = new MediaRecorder(stream);
        mediaRecorder.start();
        audioChunks = [];

        startRecordButton.disabled = true;
        stopRecordButton.disabled = false;

        mediaRecorder.ondataavailable = (event) => {
          audioChunks.push(event.data);
        };

        visualizeAudio();
      } catch (error) {
        console.error('Error starting recording:', error);
        alert('Error accessing microphone');
      }
    }

    async function stopRecording() {
      if (mediaRecorder && mediaRecorder.state === 'recording') {
        mediaRecorder.stop();
        startRecordButton.disabled = false;
        stopRecordButton.disabled = true;
        mediaRecorder.onstop = async () => {
          const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
          await transcribeAudio(audioBlob);
        };
      }
    }

    async function transcribeAudio(audioBlob) {
      try {
        const arrayBuffer = await audioBlob.arrayBuffer();
        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
        const offlineContext = new OfflineAudioContext(1, audioBuffer.length, audioBuffer.sampleRate);
        const source = offlineContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(offlineContext.destination);
        source.start();
        const renderedBuffer = await offlineContext.startRendering();
        const result = await whisperModel(renderedBuffer.getChannelData(0), {
          language: 'english',
          task: 'transcribe'
        });
        transcriptionArea.value = result.text;
        processTextButton.disabled = false;
      } catch (error) {
        console.error('Transcription error:', error);
        alert('Error during transcription');
      }
    }

    async function processWithPhi() {
      try {
        const inputText = transcriptionArea.value.trim();
        if (!inputText) {
          alert('Please provide some text to process');
          return;
        }

        // Create input tensor as per onnxruntime requirements
        const inputTensor = new ort.Tensor('string', [inputText]);

        // Run inference using the Phi-3 model
        const outputs = await phiModel.run({ input: inputTensor });
        const response = outputs.output.data[0];
        aiResponseArea.value = response;
      } catch (error) {
        console.error('Error processing with Phi-3:', error);
        alert('Error processing text with Phi-3');
      }
    }

    function visualizeAudio() {
      const canvas = visualizer;
      const canvasCtx = canvas.getContext('2d');
      const WIDTH = canvas.width;
      const HEIGHT = canvas.height;

      analyser.fftSize = 2048;
      const bufferLength = analyser.frequencyBinCount;
      const dataArray = new Uint8Array(bufferLength);

      canvasCtx.clearRect(0, 0, WIDTH, HEIGHT);

      function draw() {
        requestAnimationFrame(draw);
        analyser.getByteTimeDomainData(dataArray);
        canvasCtx.fillStyle = 'rgb(200, 200, 200)';
        canvasCtx.fillRect(0, 0, WIDTH, HEIGHT);
        canvasCtx.lineWidth = 2;
        canvasCtx.strokeStyle = 'rgb(0, 0, 0)';
        canvasCtx.beginPath();

        const sliceWidth = WIDTH * 1.0 / bufferLength;
        let x = 0;
        for (let i = 0; i < bufferLength; i++) {
          const v = dataArray[i] / 128.0;
          const y = v * HEIGHT / 2;
          if (i === 0) {
            canvasCtx.moveTo(x, y);
          } else {
            canvasCtx.lineTo(x, y);
          }
          x += sliceWidth;
        }
        canvasCtx.lineTo(canvas.width, canvas.height / 2);
        canvasCtx.stroke();
      }
      draw();
    }

    copyJSONButton.addEventListener('click', () => {
      const data = {
        transcription: transcriptionArea.value,
        response: aiResponseArea.value
      };
      navigator.clipboard.writeText(JSON.stringify(data, null, 2))
        .then(() => alert('Copied to clipboard!'))
        .catch(err => console.error('Failed to copy:', err));
    });

    document.getElementById('whisperModel').addEventListener('change', initWhisper);
    startRecordButton.addEventListener('click', startRecording);
    stopRecordButton.addEventListener('click', stopRecording);
    processTextButton.addEventListener('click', processWithPhi);

    // Initialize models on page load
    window.addEventListener('load', () => {
      initWhisper();
      initPhi3();
    });
  </script>
</body>
</html>
