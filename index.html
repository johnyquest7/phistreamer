<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Whisper & Phi-3 Transcription System</title>
  <style>
    /* (styles unchanged) */
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      display: flex;
      flex-direction: column;
      align-items: center;
      background-color: #f4f4f9;
    }
    header {
      background-color: #6200ea;
      color: white;
      padding: 1rem;
      width: 100%;
      text-align: center;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }
    main {
      display: flex;
      flex-direction: row;
      gap: 1rem;
      max-width: 1200px;
      width: 95%;
      padding: 1rem;
      margin: 1rem auto;
      background: white;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
      border-radius: 10px;
    }
    .left-panel, .right-panel {
      flex: 1;
      display: flex;
      flex-direction: column;
      gap: 1rem;
    }
    .model-status {
      padding: 1rem;
      margin: 1rem 0;
      border-radius: 5px;
      background-color: #f8f9fa;
    }
    .progress-container {
      width: 100%;
      background-color: #f0f0f0;
      border-radius: 4px;
      margin: 0.5rem 0;
    }
    .progress-bar {
      width: 0%;
      height: 20px;
      background-color: #4CAF50;
      border-radius: 4px;
      transition: width 0.3s ease-in-out;
    }
    select, button, textarea {
      width: 100%;
      padding: 0.5rem;
      margin: 0.5rem 0;
      font-size: 1rem;
      border: 1px solid #ccc;
      border-radius: 5px;
    }
    textarea {
      resize: none;
      height: 200px;
    }
    button {
      background-color: #007bff;
      color: white;
      cursor: pointer;
      transition: background-color 0.3s ease;
    }
    button:disabled {
      background-color: #cccccc;
      cursor: not-allowed;
    }
    canvas {
      width: 100%;
      height: 100px;
      border: 1px solid #ddd;
      border-radius: 5px;
    }
  </style>
</head>
<body>
  <header>
    <h1>Whisper & Phi-3 Transcription System</h1>
  </header>
  <main>
    <div class="left-panel">
      <div class="model-status">
        <h3>Model Status</h3>
        <div id="whisperStatus">
          <p>Whisper Model: <span id="whisperStatusText">Not loaded</span></p>
          <div class="progress-container">
            <div id="whisperProgress" class="progress-bar"></div>
          </div>
        </div>
        <div id="phiStatus">
          <p>Phi-3 Model: <span id="phiStatusText">Not loaded</span></p>
          <div class="progress-container">
            <div id="phiProgress" class="progress-bar"></div>
          </div>
        </div>
      </div>
      <div>
        <label for="whisperModel">Choose Whisper Model:</label>
        <select id="whisperModel">
          <option value="tiny">Tiny</option>
          <option value="small">Small</option>
          <option value="medium">Medium</option>
        </select>
      </div>
      <div>
        <canvas id="visualizer"></canvas>
        <div class="button-group">
          <button id="startRecord" disabled>Start Recording</button>
          <button id="stopRecord" disabled>Stop Recording</button>
        </div>
      </div>
      <div>
        <label for="transcription">Transcription:</label>
        <textarea id="transcription" contenteditable="true"></textarea>
      </div>
    </div>
    <div class="right-panel">
      <div>
        <label for="aiResponse">AI Response:</label>
        <textarea id="aiResponse" readonly></textarea>
      </div>
      <button id="processText" disabled>Process with Phi-3</button>
      <button id="copyJSON">Copy as JSON</button>
    </div>
  </main>

  <script type="module">
    // Import onnxruntime-web's WebGPU module using an absolute URL
    
    import * as ort from 'https://cdn.jsdelivr.net/npm/onnxruntime-web@1.20.1/dist/ort.min.js';
    import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.6.1';

    // Global variables
    let whisperModel;
    let phiModel;
    let mediaRecorder;
    let audioChunks = [];
    let audioContext;
    let analyser;

    // UI Elements
    const whisperStatusText = document.getElementById('whisperStatusText');
    const whisperProgress = document.getElementById('whisperProgress');
    const phiStatusText = document.getElementById('phiStatusText');
    const phiProgress = document.getElementById('phiProgress');
    const startRecordButton = document.getElementById('startRecord');
    const stopRecordButton = document.getElementById('stopRecord');
    const processTextButton = document.getElementById('processText');
    const transcriptionArea = document.getElementById('transcription');
    const aiResponseArea = document.getElementById('aiResponse');
    const visualizer = document.getElementById('visualizer');
    const copyJSONButton = document.getElementById('copyJSON');

    // Helper function to fetch and cache model files
    async function fetchAndCache(url) {
      try {
        const cache = await caches.open("onnx");
        let cachedResponse = await cache.match(url);
        if (!cachedResponse) {
          console.log(`${url} (network)`);
          const buffer = await fetch(url).then(response => response.arrayBuffer());
          try {
            await cache.put(url, new Response(buffer));
          } catch (error) {
            console.error(error);
          }
          return buffer;
        }
        console.log(`${url} (cached)`);
        return await cachedResponse.arrayBuffer();
      } catch (error) {
        console.error(`Can't fetch ${url}`, error);
        throw error;
      }
    }

    // Initialize the Phi-3 model using the WebGPU backend and externalData option
    async function initPhi3() {
      try {
        phiStatusText.textContent = 'Downloading Phi-3 model files...';
        // Base URL for the model files on Hugging Face
        const baseUrl = 'https://huggingface.co/onnx-community/Phi-3.5-mini-instruct-onnx-web/resolve/main/onnx/';
        const modelFile = 'model_q4f16.onnx';
        // Fetch the main model file and the external data file
        const modelBytes = await fetchAndCache(baseUrl + modelFile);
        const externalDataBytes = await fetchAndCache(baseUrl + modelFile + '_data');

        // Log model size for debugging
        const totalSizeMB = ((modelBytes.byteLength + externalDataBytes.byteLength) / 1024 / 1024).toFixed(2);
        console.log(`Model size: ${totalSizeMB} MB`);

        // Setup session options: pass the external data with its expected file path
        const options = {
          executionProviders: ['webgpu'],
          preferredOutputLocation: {},
          externalData: [
            {
              data: externalDataBytes,
              path: modelFile + '_data',
            }
          ],
          graphOptimizationLevel: 'all'
        };

        phiStatusText.textContent = 'Loading Phi-3 model...';
        phiModel = await ort.InferenceSession.create(modelBytes, options);
        phiStatusText.textContent = 'Phi-3 model ready';
        phiProgress.style.width = '100%';
        processTextButton.disabled = false;
      } catch (error) {
        console.error('Error initializing Phi-3:', error);
        phiStatusText.textContent = `Error loading Phi-3 model: ${error.message}`;
      }
    }

    // Initialize the Whisper model using transformers pipeline
    async function initWhisper() {
      try {
        const selectedModel = document.getElementById('whisperModel').value;
        const cacheKey = `whisper-${selectedModel}-downloaded`;
        whisperStatusText.textContent = localStorage.getItem(cacheKey) === 'true'
          ? 'Loading cached Whisper model...'
          : 'Downloading Whisper model...';

        whisperModel = await pipeline('automatic-speech-recognition', `Xenova/whisper-${selectedModel}`, {
          progress_callback: (progress) => {
            const percent = Math.round(progress.progress * 100);
            whisperProgress.style.width = `${percent}%`;
          }
        });

        localStorage.setItem(cacheKey, 'true');
        whisperStatusText.textContent = 'Whisper model ready';
        whisperProgress.style.width = '100%';
        startRecordButton.disabled = false;
      } catch (error) {
        console.error('Error initializing Whisper:', error);
        whisperStatusText.textContent = 'Error loading Whisper model';
      }
    }

    // Audio recording and transcription functions remain unchanged
    async function startRecording() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        analyser = audioContext.createAnalyser();
        const source = audioContext.createMediaStreamSource(stream);
        source.connect(analyser);

        mediaRecorder = new MediaRecorder(stream);
        mediaRecorder.start();
        audioChunks = [];

        startRecordButton.disabled = true;
        stopRecordButton.disabled = false;

        mediaRecorder.ondataavailable = (event) => {
          audioChunks.push(event.data);
        };

        visualizeAudio();
      } catch (error) {
        console.error('Error starting recording:', error);
        alert('Error accessing microphone');
      }
    }

    async function stopRecording() {
      if (mediaRecorder && mediaRecorder.state === 'recording') {
        mediaRecorder.stop();
        startRecordButton.disabled = false;
        stopRecordButton.disabled = true;
        mediaRecorder.onstop = async () => {
          const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
          await transcribeAudio(audioBlob);
        };
      }
    }

    async function transcribeAudio(audioBlob) {
      try {
        const arrayBuffer = await audioBlob.arrayBuffer();
        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
        const offlineContext = new OfflineAudioContext(1, audioBuffer.length, audioBuffer.sampleRate);
        const source = offlineContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(offlineContext.destination);
        source.start();
        const renderedBuffer = await offlineContext.startRendering();
        const result = await whisperModel(renderedBuffer.getChannelData(0), {
          language: 'english',
          task: 'transcribe'
        });
        transcriptionArea.value = result.text;
        processTextButton.disabled = false;
      } catch (error) {
        console.error('Transcription error:', error);
        alert('Error during transcription');
      }
    }

    async function processWithPhi() {
      try {
        const inputText = transcriptionArea.value.trim();
        if (!inputText) {
          alert('Please provide some text to process');
          return;
        }
        const inputTensor = new ort.Tensor('string', [inputText]);
        const outputs = await phiModel.run({ input: inputTensor });
        const response = outputs.output.data[0];
        aiResponseArea.value = response;
      } catch (error) {
        console.error('Error processing with Phi-3:', error);
        alert('Error processing text with Phi-3');
      }
    }

    function visualizeAudio() {
      const canvas = visualizer;
      const canvasCtx = canvas.getContext('2d');
      const WIDTH = canvas.width;
      const HEIGHT = canvas.height;

      analyser.fftSize = 2048;
      const bufferLength = analyser.frequencyBinCount;
      const dataArray = new Uint8Array(bufferLength);

      canvasCtx.clearRect(0, 0, WIDTH, HEIGHT);
      function draw() {
        requestAnimationFrame(draw);
        analyser.getByteTimeDomainData(dataArray);
        canvasCtx.fillStyle = 'rgb(200, 200, 200)';
        canvasCtx.fillRect(0, 0, WIDTH, HEIGHT);
        canvasCtx.lineWidth = 2;
        canvasCtx.strokeStyle = 'rgb(0, 0, 0)';
        canvasCtx.beginPath();

        const sliceWidth = WIDTH * 1.0 / bufferLength;
        let x = 0;
        for (let i = 0; i < bufferLength; i++) {
          const v = dataArray[i] / 128.0;
          const y = v * HEIGHT / 2;
          if (i === 0) {
            canvasCtx.moveTo(x, y);
          } else {
            canvasCtx.lineTo(x, y);
          }
          x += sliceWidth;
        }
        canvasCtx.lineTo(canvas.width, canvas.height / 2);
        canvasCtx.stroke();
      }
      draw();
    }

    copyJSONButton.addEventListener('click', () => {
      const data = {
        transcription: transcriptionArea.value,
        response: aiResponseArea.value
      };
      navigator.clipboard.writeText(JSON.stringify(data, null, 2))
        .then(() => alert('Copied to clipboard!'))
        .catch(err => console.error('Failed to copy:', err));
    });

    document.getElementById('whisperModel').addEventListener('change', initWhisper);
    startRecordButton.addEventListener('click', startRecording);
    stopRecordButton.addEventListener('click', stopRecording);
    processTextButton.addEventListener('click', processWithPhi);

    window.addEventListener('load', () => {
      initWhisper();
      initPhi3();
    });
  </script>
</body>
</html>
